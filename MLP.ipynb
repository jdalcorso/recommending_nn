{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron for recommending systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and packages\n",
    "\n",
    "Usual numerical libraries are used. In particular:\n",
    "- Numpy allows for better mathematical operations\n",
    "- Scipy is mainly used to create sparse matrices\n",
    "- Time is imported in order to track the time elapsed while running scripts\n",
    "- Keras is used in order to create and train neural networks\n",
    "- Heapq allows for a fast creation of a ranking list within the evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Input, Dense, Flatten, Embedding, Concatenate, Multiply\n",
    "from keras.models import Sequential, Model\n",
    "from keras.initializers import RandomNormal \n",
    "\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loading function has been imported by the notebooks we've seen during classes: it allows to convert a .csv file (or .dat) in a list of tuples with the form $(userid,itemid,rating)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    input_lines = []\n",
    "    users = {}\n",
    "    num_users = 0\n",
    "    items = {}\n",
    "    num_items = 0\n",
    "    raw_lines = open(filename, 'r').read().splitlines()\n",
    "    # remove the first line\n",
    "    del raw_lines[0]\n",
    "    for line in raw_lines:\n",
    "        line_content = line.split('::')\n",
    "        user_id = int(line_content[0])\n",
    "        item_id = int(line_content[1])\n",
    "        rating = float(line_content[2])\n",
    "        if user_id not in users:\n",
    "            users[user_id] = num_users\n",
    "            num_users += 1\n",
    "        if item_id not in items:\n",
    "            items[item_id] = num_items\n",
    "            num_items += 1\n",
    "        input_lines.append([users[user_id], items[item_id], rating])\n",
    "    return input_lines, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1M Movielens dataset is imported\n",
    "input_file = \"./ratings_1m.dat\"\n",
    "input_ratings, num_users, num_items = load_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "The list obtained above is processed in order to get the (sparse) `ratings` matrix. The entry $(i,j)$ of such a matrix is $1$ if $(i,j,x)$ is a tuple in the list obtained by loading the file above, $0$ otherwise. $x$ represents the rating of user i toward item j but since we're only analyzing implicit feedback we basically convert every non-zero rating as $1$. \n",
    "\n",
    "An explaination of why this is not necessarily an over-semplification of the problem is given in the report.\n",
    "\n",
    "A `test` matrix is created too. For every row (user) it contains only one non-zero entry which represents the test interaction for that user that will be used in the evaluation part below.\n",
    "\n",
    "Notice that sparse matrices not only allows to save memory but also allows to represent the matrix as a list of non-zero elements and their location. Non-sparse matrix *ratings* shouldn't be used when dealing with a massive dataset but in our case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sp.dok_matrix((num_users, num_items))\n",
    "test = sp.dok_matrix((num_users, num_items))\n",
    "        \n",
    "for i in range(len(input_ratings)):\n",
    "        if i > 0 and input_ratings[i - 1][0] == input_ratings[i][0]:\n",
    "            ratings[input_ratings[i][0], input_ratings[i][1]] = 1\n",
    "        else:\n",
    "            test[input_ratings[i][0], input_ratings[i][1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Keras is used in order to create the NNs. Both Multi-layer perceptron (MLP) and Generalized Matrix Factorization (GMF) are feed-forward neural networks. \n",
    "\n",
    "MLP structure is obtained through a .txt file. The structure  of an MLP is:\n",
    "\n",
    "input -> embedding -> hidden layers -> output \n",
    "\n",
    "The dimension of the latent space is the same for both users and items and is characterized by the first number in the .txt file divided by 2. The .txt file allows to get the dimension of the latent space for the GMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layers and their dimensions are obtained through a .txt file\n",
    "layers = open(\"MLP_architecture.txt\", \"r\")\n",
    "layers = layers.read()\n",
    "layers = layers.split(',')\n",
    "layers = list(map(int, layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_in (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_in (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_em (Embedding)             (None, 1, 32)        193280      user_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_em (Embedding)             (None, 1, 32)        118592      item_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_24 (Flatten)            (None, 32)           0           user_em[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 32)           0           item_em[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 32)           0           flatten_24[0][0]                 \n",
      "                                                                 flatten_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pred (Dense)                    (None, 1)            33          multiply_5[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 311,905\n",
      "Trainable params: 311,905\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This boolean variable states whether to build a GMF or a MLP\n",
    "# If GMF = True, hidden layers are skipped and embedding layers are multiplied instead of concatenated.\n",
    "GMF = False\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int32', name = 'user_in')\n",
    "item_in = Input(shape=(1,), dtype='int32', name = 'item_in')\n",
    "\n",
    "Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = 'user_em',\n",
    "                                  embeddings_initializer = RandomNormal(), embeddings_regularizer = l2(0),\n",
    "                                   input_length=1)\n",
    "Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'item_em',\n",
    "                                  embeddings_initializer = RandomNormal(), embeddings_regularizer = l2(0),\n",
    "                                   input_length=1)   \n",
    "    \n",
    "# Embedding layers are flattened because they return matrices but not in this case (one-hot encoding)    \n",
    "user_latent = Flatten()(Embedding_User(user_in))\n",
    "item_latent = Flatten()(Embedding_Item(item_in))\n",
    "\n",
    "if GMF == False:\n",
    "    vector = Concatenate()([user_latent, item_latent])\n",
    "else:\n",
    "    vector = Multiply()([user_latent, item_latent])\n",
    "\n",
    "# hidden layers\n",
    "if GMF == False:\n",
    "    for n in range(len(layers)):\n",
    "        layer = Dense(layers[n], kernel_regularizer = l2(0), activation='relu')\n",
    "        vector = layer(vector)\n",
    "        \n",
    "# output layer\n",
    "prediction = Dense(1, activation='sigmoid', name = 'pred')(vector)\n",
    "    \n",
    "model = Model(inputs =[user_in, item_in], outputs = prediction)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We train the models with all the positive interactions between users and items ($1$ in ratings matrix) and we also pick some negative interactions (4 negative interactions for each positive one).\n",
    "Doing this we almost follow the training process of the Neural Collaborative Filtering article in bibliography.\n",
    "\n",
    "So after obtaining all the training instances, which are tuples $(userid, itemid, rating)$ where $rating$ is boolean we use the fit function of keras in order to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19418/19418 [==============================] - 18s 906us/step - loss: 0.3356\n",
      "19418/19418 [==============================] - 18s 915us/step - loss: 0.2657\n",
      "Elapsed time: 66.05591201782227\n"
     ]
    }
   ],
   "source": [
    "# These parameters are fixed.\n",
    "n_epochs = 10\n",
    "batch_size = 256\n",
    "t = time.time()\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    user_train, item_train, labels_train = [], [], []\n",
    "    num_neg = 4\n",
    "\n",
    "    for (u, i) in ratings.keys():\n",
    "    \n",
    "        # Training uses all the ratings (i.e. 1 in the rating matrix)\n",
    "        user_train.append(u)\n",
    "        item_train.append(i)\n",
    "        labels_train.append(1)\n",
    "        \n",
    "        # Also for each rating used, some random couples user-item with label 0 (item not rated) are added ..\n",
    "        for k in range(num_neg):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in ratings:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_train.append(u)\n",
    "            item_train.append(j)\n",
    "            labels_train.append(0)\n",
    "\n",
    "    train_history = model.fit([np.array(user_train), np.array(item_train)], np.array(labels_train),\n",
    "                             batch_size=batch_size, epochs = 1, shuffle=True)\n",
    "\n",
    "print('Elapsed time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluation is made for each user. For a better explaination of this part see the report and the other notebook involving CF.\n",
    "\n",
    "In general the idea is to evaluate a single user picking 1 positive interaction (test interaction) and 99 negative ones. Then we predict the score for each of these 100 interactions and we evaluate wheter the positive one is in the top-10: if this is the case, we talk about an *hit*.\n",
    "\n",
    "Then the *hit ratio* is the number of hits divided by the total number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_user_k(k, ratings, test, model):\n",
    "    num_neg = 99\n",
    "    \n",
    "    \n",
    "    # two vectors are created\n",
    "    # users contains 100 times the userid\n",
    "    # items contains the itemid of the items (both the positive and negatives) involved\n",
    "    positive = list(test.keys())[k]\n",
    "    negatives = []\n",
    "    \n",
    "    for i in range(num_neg):\n",
    "        j = np.random.randint(num_items)\n",
    "        while (k,j) in ratings.keys():\n",
    "            j = np.random.randint(num_items)\n",
    "        negatives.append((k,j))\n",
    "    \n",
    "    users = np.full(num_neg + 1 , k, dtype = 'int32')\n",
    "    items = [positive[1]]\n",
    "    for i in range(num_neg):\n",
    "        items.append(negatives[i][1])\n",
    "    \n",
    "    # scores are predicted using keras\n",
    "    scores = model.predict([users, np.array(items)], batch_size=100, verbose=0)\n",
    "    \n",
    "    item_score_dict = {}\n",
    "    for i in range(num_neg + 1):\n",
    "        item = items[i]\n",
    "        item_score_dict[item] = scores[i]\n",
    "    \n",
    "    # Heapq allows a fast creation for the top-10 scoreboard using a dictionary\n",
    "    ranklist = heapq.nlargest(10, item_score_dict, key = item_score_dict.get)\n",
    "    \n",
    "    #returns 1 if the positive is in top-10, 0 otherwise\n",
    "    hr = 0\n",
    "    for item in ranklist:\n",
    "        if item == positive[1]:\n",
    "            hr = 1\n",
    "    \n",
    "    return hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Ratio: 0.7021666666666667\n",
      "Elapsed time: 150.94761633872986\n"
     ]
    }
   ],
   "source": [
    "# Finally the HR (Hit Ratio) is calculated dividing the number of hits by the total number of users.\n",
    "s = 0\n",
    "t = time.time()\n",
    "\n",
    "for i in range(num_users):\n",
    "    s += evaluate_user_k(i, ratings, test, model)\n",
    "print('Hit Ratio:', s/num_users)\n",
    "print('Elapsed time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the notebook has been used to test and gain data in order to write the report.\n",
    "\n",
    "Parameters such as the .txt file and the GMF boolean variable has been changed according to each model.\n",
    "For a complete overview of all the models and parameters please see the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
