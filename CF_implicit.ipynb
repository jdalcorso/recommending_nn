{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering on implicit feedback dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and packages\n",
    "Usual numerical libraries are used. In particular:\n",
    "- Numpy allows for better mathematical operations\n",
    "- Scipy is mainly used to create sparse matrices\n",
    "- Time is imported in order to track the time elapsed while running scripts\n",
    "- Implicit contains fast methods to deal with recommending systems over implicit feedback datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loading function has been imported by the notebooks we've seen during classes: it allows to convert a .csv file (or .dat) in a list of tuples with the form $(userid, itemid, rating)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    input_lines = []\n",
    "    users = {}\n",
    "    num_users = 0\n",
    "    items = {}\n",
    "    num_items = 0\n",
    "    raw_lines = open(filename, 'r').read().splitlines()\n",
    "    # remove the first line\n",
    "    del raw_lines[0]\n",
    "    for line in raw_lines:\n",
    "        line_content = line.split('::')\n",
    "        user_id = int(line_content[0])\n",
    "        item_id = int(line_content[1])\n",
    "        rating = float(line_content[2])\n",
    "        if user_id not in users:\n",
    "            users[user_id] = num_users\n",
    "            num_users += 1\n",
    "        if item_id not in items:\n",
    "            items[item_id] = num_items\n",
    "            num_items += 1\n",
    "        input_lines.append([users[user_id], items[item_id], rating])\n",
    "    return input_lines, num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1m Movielens dataset is the input file\n",
    "input_file = \"./ratings_1m.dat\"\n",
    "input_ratings, num_users, num_items = load_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "The list obtained above is processed in order to get the (sparse) `ratings` matrix. The entry $(i,j)$ of such a matrix is $1$ if $(i,j,x)$ is a tuple in the list obtained by loading the file above, $0$ otherwise. $x$ represents the rating of user i toward item j but since we're only analyzing implicit feedback we basically convert every non-zero rating as $1$. \n",
    "\n",
    "An explaination of why this is not necessarily an over-semplification of the problem is given in the report.\n",
    "\n",
    "A `test` matrix is created too. For every row (user) it contains only one non-zero entry which represents the test interaction for that user that will be used in the evaluation part below.\n",
    "\n",
    "Notice that sparse matrices not only allows to save memory but also allows to represent the matrix as a list of non-zero elements and their location. Non-sparse matrix *ratings* shouldn't be used when dealing with a massive dataset but in our case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = sp.dok_matrix((num_users, num_items))\n",
    "test = sp.dok_matrix((num_users, num_items))\n",
    "\n",
    "\n",
    "# This cycle allows to put every interaction (user,item) in the ratings matrix except one for each user,\n",
    "# which is used as test.\n",
    "for i in range(len(input_ratings)):\n",
    "        if i > 0 and input_ratings[i - 1][0] == input_ratings[i][0]:\n",
    "            ratings[input_ratings[i][0], input_ratings[i][1]] = 1\n",
    "        else:\n",
    "            test[input_ratings[i][0], input_ratings[i][1]] = 1\n",
    "\n",
    "# We transpose in order to obtain a matrix with dimensions (num_items, num_users) which\n",
    "# is the standard format for the implicit library.\n",
    "ratings = ratings.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The NearestNeighbor model is created using the library `implicit`.\n",
    "Different classes can be used and they differs in how similarity is calculated.\n",
    "For example `ItemItemRecommender` allows to calculate similarities using Euclidean distance while `CosineRecommender` uses (normalized) Cosine similarity.\n",
    "\n",
    "The idea is to create a model and to fit it to the ratings matrix created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d817730eca094c95adad5e513752d275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3706.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = implicit.nearest_neighbours.ItemItemRecommender(K = 10)\n",
    "model.fit(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the CF models using Hit Ratio (HR). It is commonly used in literature (see the report for references).\n",
    "\n",
    "For each user we randomly select 100 items: 1 of them is the test interaction and the other 99 are randomly selected among items that has not been rated by that user. Then we calculate the score of the selected user for each of these items using our model, we create a ranking list and $HR=1$ if the test interaction item (i.e. `positive`) is in the top-10, $0$ otherwise.\n",
    "\n",
    "Basically we have a hit ($HR=1$) when a positive (test) interaction is (almost, in a top-10) correctly detected by the model among many other non-interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matrix is used by the function of implicit library. It is only the transpose of ratings matrix\n",
    "# It is passed as a parameter in order to not re-calculate it at every call of the evaluation function.\n",
    "user_items = ratings.T.tocsr()\n",
    "\n",
    "def evaluate_user_k(k, ratings, test, model, user_items):\n",
    "    \n",
    "    num_neg = 99\n",
    "    \n",
    "    positive = list(test.keys())[k][1]\n",
    "    negatives = []\n",
    "    \n",
    "    items = []\n",
    "    items.append(positive)\n",
    "    \n",
    "    for i in range(num_neg):\n",
    "        j = np.random.randint(num_items)\n",
    "        while (k,j) in ratings.keys():\n",
    "            j = np.random.randint(num_items)\n",
    "        items.append(j)\n",
    "    \n",
    "    # create the scoreboard and keep track of the top-10\n",
    "    ranklist =  model.rank_items(k, user_items, items)\n",
    "    ranklist = ranklist[:10]\n",
    "    \n",
    "    # calculate whether we have an hit\n",
    "    hr = 0\n",
    "    for item in ranklist:\n",
    "         if item[0] == positive and item[1] > 0:\n",
    "                hr = 1\n",
    "                \n",
    "    return hr   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last cell we actually test the model for each user.\n",
    "We keep track of the elapsed time and we make a ratio between the number of hits and the total number of users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users processed\n",
      "1000 users processed\n",
      "2000 users processed\n",
      "3000 users processed\n",
      "4000 users processed\n",
      "5000 users processed\n",
      "6000 users processed\n",
      "Hit Ratio: 0.16804635761589404\n",
      "Elapsed time: 135.52697610855103\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "t = time.time()\n",
    "\n",
    "for i in range(num_users):\n",
    "    s += evaluate_user_k(i, ratings, test, model, user_items)\n",
    "    if i%1000 == 0:\n",
    "        print(i,'users processed')\n",
    "        \n",
    "print('Hit Ratio:', s/num_users)\n",
    "print('Elapsed time:', time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this notebook has been used to test models and gain data in order to make a report.\n",
    "Parameters and functions changes according to which model has been used. For a complete overview of models and parameters please see the report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
